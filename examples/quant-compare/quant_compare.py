results = {
    "bfp16": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="bfp16", collect_midlayers=False)""",
        "output": {'mmlu': {'acc,none': 0.6386554621848739, 'acc_stderr,none': 0.0038313860982312955, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5810839532412327, 'acc_stderr,none': 0.006753048888918861}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.49206349206349204, 'acc_stderr,none': 0.044715725362943486}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7393939393939394, 'acc_stderr,none': 0.034277431758165236}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8382352941176471, 'acc_stderr,none': 0.025845017986926924}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8396624472573839, 'acc_stderr,none': 0.023884380925965676}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.768595041322314, 'acc_stderr,none': 0.03849856098794088}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252627}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.754601226993865, 'acc_stderr,none': 0.03380939813943354}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6936416184971098, 'acc_stderr,none': 0.024818350129436593}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.3318435754189944, 'acc_stderr,none': 0.015748421208187303}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.7106109324758842, 'acc_stderr,none': 0.025755865922632924}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7283950617283951, 'acc_stderr,none': 0.024748624490537375}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4915254237288136, 'acc_stderr,none': 0.01276840169726906}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7777777777777778, 'acc_stderr,none': 0.03188578017686398}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.7164467331831349, 'acc_stderr,none': 0.007804066544695224}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.66, 'acc_stderr,none': 0.04760952285695237}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.720754716981132, 'acc_stderr,none': 0.027611163402399715}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6416184971098265, 'acc_stderr,none': 0.03656343653353159}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.37, 'acc_stderr,none': 0.04852365870939098}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6905829596412556, 'acc_stderr,none': 0.03102441174057221}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8252427184466019, 'acc_stderr,none': 0.037601780060266196}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8846153846153846, 'acc_stderr,none': 0.02093019318517933}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653695}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.8084291187739464, 'acc_stderr,none': 0.014072859310451949}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7352941176470589, 'acc_stderr,none': 0.025261691219729487}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.5319148936170213, 'acc_stderr,none': 0.029766675075873866}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.7352941176470589, 'acc_stderr,none': 0.02679956202488768}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5120481927710844, 'acc_stderr,none': 0.03891364495835817}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7419564510887228, 'acc_stderr,none': 0.007724217878772125}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.5, 'acc_stderr,none': 0.047036043419179864}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7777777777777778, 'acc_stderr,none': 0.02962022787479047}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8808290155440415, 'acc_stderr,none': 0.023381935348121427}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6435897435897436, 'acc_stderr,none': 0.0242831405294673}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.7310924369747899, 'acc_stderr,none': 0.028801392193631276}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8275229357798165, 'acc_stderr,none': 0.01619780795684805}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7786259541984732, 'acc_stderr,none': 0.03641297081313728}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.673202614379085, 'acc_stderr,none': 0.01897542792050721}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.7346938775510204, 'acc_stderr,none': 0.02826388994378461}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8606965174129353, 'acc_stderr,none': 0.024484487162913973}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.86, 'acc_stderr,none': 0.03487350880197769}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5470980019029495, 'acc_stderr,none': 0.00858158838354017}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.34, 'acc_stderr,none': 0.04760952285695236}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6370370370370371, 'acc_stderr,none': 0.04153948404742399}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.7039473684210527, 'acc_stderr,none': 0.03715062154998904}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7361111111111112, 'acc_stderr,none': 0.03685651095897532}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.45, 'acc_stderr,none': 0.05}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.52, 'acc_stderr,none': 0.050211673156867795}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.35, 'acc_stderr,none': 0.04793724854411019}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.5098039215686274, 'acc_stderr,none': 0.04974229460422817}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.74, 'acc_stderr,none': 0.04408440022768079}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5531914893617021, 'acc_stderr,none': 0.0325005368436584}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6413793103448275, 'acc_stderr,none': 0.03996629574876719}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.47619047619047616, 'acc_stderr,none': 0.025722097064388525}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7645161290322581, 'acc_stderr,none': 0.024137632429337703}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.45320197044334976, 'acc_stderr,none': 0.03502544650845872}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.68, 'acc_stderr,none': 0.04688261722621505}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.3888888888888889, 'acc_stderr,none': 0.029723278961476668}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.46357615894039733, 'acc_stderr,none': 0.04071636065944216}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.49074074074074076, 'acc_stderr,none': 0.034093869469927006}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.4732142857142857, 'acc_stderr,none': 0.047389751192741546}}
    },
    "fp16": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="fp16", collect_midlayers=False)""",
        "output": {'mmlu': {'acc,none': 0.6383706024782795, 'acc_stderr,none': 0.003831422558473383, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5793836344314559, 'acc_stderr,none': 0.006756809261060399}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.49206349206349204, 'acc_stderr,none': 0.044715725362943486}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7393939393939394, 'acc_stderr,none': 0.034277431758165236}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8382352941176471, 'acc_stderr,none': 0.025845017986926924}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8312236286919831, 'acc_stderr,none': 0.02438140683258623}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7603305785123967, 'acc_stderr,none': 0.038968789850704164}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252627}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.754601226993865, 'acc_stderr,none': 0.03380939813943354}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.684971098265896, 'acc_stderr,none': 0.025009313790069723}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.32849162011173183, 'acc_stderr,none': 0.01570793539849645}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.7106109324758842, 'acc_stderr,none': 0.025755865922632924}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7376543209876543, 'acc_stderr,none': 0.024477222856135114}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4908735332464146, 'acc_stderr,none': 0.01276810860164001}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7719298245614035, 'acc_stderr,none': 0.03218093795602357}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.7177341486964918, 'acc_stderr,none': 0.007785426664357011}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.66, 'acc_stderr,none': 0.04760952285695237}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7132075471698113, 'acc_stderr,none': 0.02783491252754407}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.653179190751445, 'acc_stderr,none': 0.036291466701596636}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.37, 'acc_stderr,none': 0.04852365870939098}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6905829596412556, 'acc_stderr,none': 0.03102441174057221}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8155339805825242, 'acc_stderr,none': 0.03840423627288276}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8931623931623932, 'acc_stderr,none': 0.02023714900899092}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653695}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.8071519795657727, 'acc_stderr,none': 0.014108533515757431}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7450980392156863, 'acc_stderr,none': 0.024954184324879912}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.524822695035461, 'acc_stderr,none': 0.02979071924382972}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.7463235294117647, 'acc_stderr,none': 0.02643132987078954}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5120481927710844, 'acc_stderr,none': 0.03891364495835817}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7416314592135197, 'acc_stderr,none': 0.007733813805389864}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.5, 'acc_stderr,none': 0.047036043419179864}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7777777777777778, 'acc_stderr,none': 0.02962022787479047}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8704663212435233, 'acc_stderr,none': 0.024233532297758712}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6461538461538462, 'acc_stderr,none': 0.024243783994062164}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.726890756302521, 'acc_stderr,none': 0.028942004040998167}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8275229357798165, 'acc_stderr,none': 0.01619780795684805}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7709923664122137, 'acc_stderr,none': 0.036853466317118506}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6748366013071896, 'acc_stderr,none': 0.018950886770806308}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.7387755102040816, 'acc_stderr,none': 0.02812342933514279}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8606965174129353, 'acc_stderr,none': 0.024484487162913973}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.86, 'acc_stderr,none': 0.03487350880197769}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5474151601649223, 'acc_stderr,none': 0.008583528702418684}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.35, 'acc_stderr,none': 0.0479372485441102}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6222222222222222, 'acc_stderr,none': 0.04188307537595853}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6973684210526315, 'acc_stderr,none': 0.037385206761196686}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7361111111111112, 'acc_stderr,none': 0.03685651095897532}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.44, 'acc_stderr,none': 0.04988876515698589}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620333}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.35, 'acc_stderr,none': 0.04793724854411019}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.5, 'acc_stderr,none': 0.04975185951049946}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.74, 'acc_stderr,none': 0.044084400227680794}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.548936170212766, 'acc_stderr,none': 0.032529096196131965}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6413793103448275, 'acc_stderr,none': 0.03996629574876719}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.47619047619047616, 'acc_stderr,none': 0.025722097064388525}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7645161290322581, 'acc_stderr,none': 0.024137632429337703}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.458128078817734, 'acc_stderr,none': 0.03505630140785741}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.69, 'acc_stderr,none': 0.04648231987117316}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.37777777777777777, 'acc_stderr,none': 0.02956070739246571}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.4966887417218543, 'acc_stderr,none': 0.04082393379449654}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.49074074074074076, 'acc_stderr,none': 0.034093869469927006}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.4732142857142857, 'acc_stderr,none': 0.047389751192741546}}
    },
    "8bit-hqq": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="hqq8", add_hooks=False, model_device="cuda")""",
        "output": {'mmlu': {'acc,none': 0.6387266771115225, 'acc_stderr,none': 0.0038327819968707387, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5804463336875664, 'acc_stderr,none': 0.006750246636115241}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.49206349206349204, 'acc_stderr,none': 0.044715725362943486}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7393939393939394, 'acc_stderr,none': 0.034277431758165236}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8382352941176471, 'acc_stderr,none': 0.025845017986926924}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8396624472573839, 'acc_stderr,none': 0.023884380925965676}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7768595041322314, 'acc_stderr,none': 0.03800754475228733}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252627}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.754601226993865, 'acc_stderr,none': 0.03380939813943354}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6907514450867052, 'acc_stderr,none': 0.024883140570071755}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.3329608938547486, 'acc_stderr,none': 0.015761716178397563}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.7138263665594855, 'acc_stderr,none': 0.025670259242188957}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7314814814814815, 'acc_stderr,none': 0.024659685185967294}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4876140808344198, 'acc_stderr,none': 0.012766317315473567}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7777777777777778, 'acc_stderr,none': 0.03188578017686398}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.7199871258448665, 'acc_stderr,none': 0.007777862906058198}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.66, 'acc_stderr,none': 0.04760952285695237}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7169811320754716, 'acc_stderr,none': 0.027724236492700918}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6473988439306358, 'acc_stderr,none': 0.03643037168958548}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.37, 'acc_stderr,none': 0.04852365870939098}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.695067264573991, 'acc_stderr,none': 0.030898610882477515}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8252427184466019, 'acc_stderr,none': 0.037601780060266196}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8888888888888888, 'acc_stderr,none': 0.020588491316092368}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653695}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.8071519795657727, 'acc_stderr,none': 0.014108533515757431}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7516339869281046, 'acc_stderr,none': 0.02473998135511359}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.5354609929078015, 'acc_stderr,none': 0.02975238965742705}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.7426470588235294, 'acc_stderr,none': 0.026556519470041517}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5240963855421686, 'acc_stderr,none': 0.038879718495972646}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7387065323366916, 'acc_stderr,none': 0.007761507168364032}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.5, 'acc_stderr,none': 0.047036043419179864}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7828282828282829, 'acc_stderr,none': 0.029376616484945637}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8704663212435233, 'acc_stderr,none': 0.024233532297758712}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6461538461538462, 'acc_stderr,none': 0.024243783994062164}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.7058823529411765, 'acc_stderr,none': 0.029597329730978096}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8293577981651377, 'acc_stderr,none': 0.016129271025099878}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7633587786259542, 'acc_stderr,none': 0.03727673575596915}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6715686274509803, 'acc_stderr,none': 0.01899970738316267}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.7346938775510204, 'acc_stderr,none': 0.02826388994378461}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8606965174129353, 'acc_stderr,none': 0.024484487162913973}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.84, 'acc_stderr,none': 0.03684529491774708}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5480494766888677, 'acc_stderr,none': 0.008589915933834876}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.32, 'acc_stderr,none': 0.046882617226215034}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6222222222222222, 'acc_stderr,none': 0.04188307537595853}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6907894736842105, 'acc_stderr,none': 0.037610708698674805}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7291666666666666, 'acc_stderr,none': 0.03716177437566016}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.43, 'acc_stderr,none': 0.049756985195624284}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.51, 'acc_stderr,none': 0.05024183937956911}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.34, 'acc_stderr,none': 0.04760952285695235}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.5, 'acc_stderr,none': 0.04975185951049946}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.74, 'acc_stderr,none': 0.044084400227680794}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5617021276595745, 'acc_stderr,none': 0.03243618636108101}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6413793103448275, 'acc_stderr,none': 0.03996629574876719}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.47883597883597884, 'acc_stderr,none': 0.025728230952130726}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7612903225806451, 'acc_stderr,none': 0.024251071262208834}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.4630541871921182, 'acc_stderr,none': 0.035083705204426656}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.69, 'acc_stderr,none': 0.04648231987117316}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.3962962962962963, 'acc_stderr,none': 0.029822619458534}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.4966887417218543, 'acc_stderr,none': 0.04082393379449654}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.5, 'acc_stderr,none': 0.034099716973523674}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.48214285714285715, 'acc_stderr,none': 0.047427623612430116}}
    },
    "8bit-bnb": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="int8", collect_midlayers=False)""",
        "output": {'mmlu': {'acc,none': 0.6305369605469306, 'acc_stderr,none': 0.0038434827279743345, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.571519659936238, 'acc_stderr,none': 0.00673629421897851}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.46825396825396826, 'acc_stderr,none': 0.04463112720677172}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7393939393939394, 'acc_stderr,none': 0.034277431758165236}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8382352941176471, 'acc_stderr,none': 0.025845017986926924}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8185654008438819, 'acc_stderr,none': 0.025085961144579654}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7768595041322314, 'acc_stderr,none': 0.03800754475228733}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.75, 'acc_stderr,none': 0.04186091791394607}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7852760736196319, 'acc_stderr,none': 0.03226219377286774}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6994219653179191, 'acc_stderr,none': 0.0246853168672578}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.3106145251396648, 'acc_stderr,none': 0.015476515438005567}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.6977491961414791, 'acc_stderr,none': 0.02608270069539966}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7283950617283951, 'acc_stderr,none': 0.024748624490537375}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4791395045632334, 'acc_stderr,none': 0.012759117066518019}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7719298245614035, 'acc_stderr,none': 0.03218093795602357}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.708400386224654, 'acc_stderr,none': 0.007889486269315779}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.64, 'acc_stderr,none': 0.04824181513244218}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7132075471698113, 'acc_stderr,none': 0.02783491252754407}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6416184971098265, 'acc_stderr,none': 0.036563436533531585}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.41, 'acc_stderr,none': 0.049431107042371025}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6816143497757847, 'acc_stderr,none': 0.03126580522513713}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8349514563106796, 'acc_stderr,none': 0.036756688322331886}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8803418803418803, 'acc_stderr,none': 0.02126271940040697}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.83, 'acc_stderr,none': 0.03775251680686371}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7956577266922095, 'acc_stderr,none': 0.014419123980931899}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7254901960784313, 'acc_stderr,none': 0.025553169991826507}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.5106382978723404, 'acc_stderr,none': 0.02982074719142244}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.7132352941176471, 'acc_stderr,none': 0.02747227447323382}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5180722891566265, 'acc_stderr,none': 0.03889951252827217}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7377315567110823, 'acc_stderr,none': 0.007776032530966195}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.49122807017543857, 'acc_stderr,none': 0.04702880432049615}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7878787878787878, 'acc_stderr,none': 0.029126522834586818}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8549222797927462, 'acc_stderr,none': 0.025416343096306433}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6410256410256411, 'acc_stderr,none': 0.024321738484602364}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.7226890756302521, 'acc_stderr,none': 0.029079374539480007}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8293577981651377, 'acc_stderr,none': 0.016129271025099888}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7862595419847328, 'acc_stderr,none': 0.0359546161177469}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.673202614379085, 'acc_stderr,none': 0.018975427920507215}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.726530612244898, 'acc_stderr,none': 0.028535560337128438}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8507462686567164, 'acc_stderr,none': 0.02519692987482708}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653693}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5372660957817951, 'acc_stderr,none': 0.008597781102765123}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.34, 'acc_stderr,none': 0.04760952285695236}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6296296296296297, 'acc_stderr,none': 0.041716541613545426}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6973684210526315, 'acc_stderr,none': 0.037385206761196686}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7291666666666666, 'acc_stderr,none': 0.03716177437566016}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.43, 'acc_stderr,none': 0.04975698519562428}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.51, 'acc_stderr,none': 0.05024183937956912}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252604}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.47058823529411764, 'acc_stderr,none': 0.049665709039785295}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5404255319148936, 'acc_stderr,none': 0.03257901482099835}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6137931034482759, 'acc_stderr,none': 0.04057324734419036}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.4497354497354497, 'acc_stderr,none': 0.02562085704293665}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7451612903225806, 'acc_stderr,none': 0.0247901184593322}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.4827586206896552, 'acc_stderr,none': 0.035158955511657}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.68, 'acc_stderr,none': 0.04688261722621505}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.37407407407407406, 'acc_stderr,none': 0.02950286112895529}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.45695364238410596, 'acc_stderr,none': 0.04067325174247442}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.49537037037037035, 'acc_stderr,none': 0.03409825519163572}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.48214285714285715, 'acc_stderr,none': 0.047427623612430116}}
    },
    "8bit-gptq": {
        "command": """m = Model("astronomer/Llama-3-8B-Instruct-GPTQ-8-Bit", dtype="fp16", add_hooks=False)""",
        "output": {'mmlu': {'acc,none': 0.30309072781655033, 'acc_stderr,none': 0.0037328722222367714, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.37236981934112645, 'acc_stderr,none': 0.006600352362986616}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.2857142857142857, 'acc_stderr,none': 0.0404061017820884}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7090909090909091, 'acc_stderr,none': 0.03546563019624336}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.7794117647058824, 'acc_stderr,none': 0.029102254389674093}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8016877637130801, 'acc_stderr,none': 0.025955020841621112}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.30578512396694213, 'acc_stderr,none': 0.04205953933884122}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.26851851851851855, 'acc_stderr,none': 0.04284467968052192}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.25153374233128833, 'acc_stderr,none': 0.034089978868575295}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.26011560693641617, 'acc_stderr,none': 0.023618678310069363}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.24134078212290502, 'acc_stderr,none': 0.014310999547961452}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.21221864951768488, 'acc_stderr,none': 0.023222756797435122}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.24074074074074073, 'acc_stderr,none': 0.023788583551658533}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.41590612777053454, 'acc_stderr,none': 0.012588323850313613}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.3216374269005848, 'acc_stderr,none': 0.03582529442573122}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.28934663662697135, 'acc_stderr,none': 0.00796336522298877}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.31, 'acc_stderr,none': 0.04648231987117317}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.21132075471698114, 'acc_stderr,none': 0.025125766484827856}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.24855491329479767, 'acc_stderr,none': 0.03295304696818318}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.19, 'acc_stderr,none': 0.039427724440366234}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.31390134529147984, 'acc_stderr,none': 0.03114679648297246}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.17475728155339806, 'acc_stderr,none': 0.03760178006026621}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.2905982905982906, 'acc_stderr,none': 0.029745048572674057}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.3, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.25287356321839083, 'acc_stderr,none': 0.015543377313719681}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.24509803921568626, 'acc_stderr,none': 0.024630048979824775}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.3049645390070922, 'acc_stderr,none': 0.027464708442022135}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.5808823529411765, 'acc_stderr,none': 0.029972807170464622}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.28313253012048195, 'acc_stderr,none': 0.03507295431370518}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.2622684432889178, 'acc_stderr,none': 0.00782370322319169}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.2982456140350877, 'acc_stderr,none': 0.04303684033537316}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.18181818181818182, 'acc_stderr,none': 0.027479603010538804}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.22279792746113988, 'acc_stderr,none': 0.03003114797764154}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.19743589743589743, 'acc_stderr,none': 0.020182646968674823}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.23109243697478993, 'acc_stderr,none': 0.027381406927868963}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.22568807339449543, 'acc_stderr,none': 0.017923087667803057}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.26717557251908397, 'acc_stderr,none': 0.038808483010823965}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.2908496732026144, 'acc_stderr,none': 0.018373116915903966}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.24545454545454545, 'acc_stderr,none': 0.041220665028782834}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.49387755102040815, 'acc_stderr,none': 0.03200682020163907}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.24875621890547264, 'acc_stderr,none': 0.030567675938916718}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.28, 'acc_stderr,none': 0.045126085985421276}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.2530922930542341, 'acc_stderr,none': 0.0077131048563821416}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.22, 'acc_stderr,none': 0.04163331998932269}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.18518518518518517, 'acc_stderr,none': 0.03355677216313142}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.19736842105263158, 'acc_stderr,none': 0.03238981601699397}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.2847222222222222, 'acc_stderr,none': 0.03773809990686935}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.23, 'acc_stderr,none': 0.042295258468165065}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252605}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.22, 'acc_stderr,none': 0.0416333199893227}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.19607843137254902, 'acc_stderr,none': 0.03950581861179961}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.29, 'acc_stderr,none': 0.045604802157206845}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.26382978723404255, 'acc_stderr,none': 0.02880998985410298}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.2413793103448276, 'acc_stderr,none': 0.03565998174135302}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.21957671957671956, 'acc_stderr,none': 0.02132001859977037}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.25483870967741934, 'acc_stderr,none': 0.024790118459332215}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.23645320197044334, 'acc_stderr,none': 0.029896114291733552}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.42, 'acc_stderr,none': 0.049604496374885836}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.2111111111111111, 'acc_stderr,none': 0.024882116857655078}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.2781456953642384, 'acc_stderr,none': 0.03658603262763743}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.30092592592592593, 'acc_stderr,none': 0.03128039084329881}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.35714285714285715, 'acc_stderr,none': 0.04547960999764376}}
    },
    "4bit-hqq": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="hqq4", add_hooks=False, model_device="cuda")""",
        "output": {'mmlu': {'acc,none': 0.6229169633955277, 'acc_stderr,none': 0.0038749122904357144, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5681190223166844, 'acc_stderr,none': 0.006797515295802918}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.5, 'acc_stderr,none': 0.04472135954999579}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7212121212121212, 'acc_stderr,none': 0.03501438706296781}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.7892156862745098, 'acc_stderr,none': 0.028626547912437406}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8143459915611815, 'acc_stderr,none': 0.025310495376944867}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.768595041322314, 'acc_stderr,none': 0.0384985609879409}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252627}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7423312883435583, 'acc_stderr,none': 0.03436150827846917}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.7052023121387283, 'acc_stderr,none': 0.024547617794803835}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.3340782122905028, 'acc_stderr,none': 0.015774911422381632}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.7202572347266881, 'acc_stderr,none': 0.0254942593506949}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7067901234567902, 'acc_stderr,none': 0.025329888171900922}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4654498044328553, 'acc_stderr,none': 0.012739711554045696}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7602339181286549, 'acc_stderr,none': 0.03274485211946956}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.698744769874477, 'acc_stderr,none': 0.007982556187252559}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.68, 'acc_stderr,none': 0.04688261722621504}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7094339622641509, 'acc_stderr,none': 0.02794321998933712}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6473988439306358, 'acc_stderr,none': 0.036430371689585475}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.37, 'acc_stderr,none': 0.04852365870939099}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6636771300448431, 'acc_stderr,none': 0.031708824268455}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8058252427184466, 'acc_stderr,none': 0.03916667762822582}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8717948717948718, 'acc_stderr,none': 0.021901905115073318}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.79, 'acc_stderr,none': 0.040936018074033256}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7867177522349936, 'acc_stderr,none': 0.014648172749593529}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7124183006535948, 'acc_stderr,none': 0.02591780611714716}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.5212765957446809, 'acc_stderr,none': 0.029800481645628693}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6838235294117647, 'acc_stderr,none': 0.028245687391462906}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5120481927710844, 'acc_stderr,none': 0.03891364495835817}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7237569060773481, 'acc_stderr,none': 0.007864706634137282}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.4298245614035088, 'acc_stderr,none': 0.04657047260594963}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7727272727272727, 'acc_stderr,none': 0.0298575156733864}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8704663212435233, 'acc_stderr,none': 0.024233532297758712}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6256410256410256, 'acc_stderr,none': 0.024537591572830513}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.6890756302521008, 'acc_stderr,none': 0.030066761582977927}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8275229357798165, 'acc_stderr,none': 0.016197807956848047}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7404580152671756, 'acc_stderr,none': 0.03844876139785271}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6519607843137255, 'acc_stderr,none': 0.019270998708223977}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.7224489795918367, 'acc_stderr,none': 0.02866685779027465}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.835820895522388, 'acc_stderr,none': 0.02619392354445413}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.84, 'acc_stderr,none': 0.036845294917747094}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.531557247066286, 'acc_stderr,none': 0.008610051386792187}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.31, 'acc_stderr,none': 0.04648231987117316}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6, 'acc_stderr,none': 0.04232073695151589}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6447368421052632, 'acc_stderr,none': 0.03894734487013316}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7638888888888888, 'acc_stderr,none': 0.03551446610810826}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001975}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.53, 'acc_stderr,none': 0.050161355804659205}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.35, 'acc_stderr,none': 0.04793724854411019}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.49019607843137253, 'acc_stderr,none': 0.04974229460422817}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5404255319148936, 'acc_stderr,none': 0.03257901482099835}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6275862068965518, 'acc_stderr,none': 0.04028731532947558}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.42857142857142855, 'acc_stderr,none': 0.025487187147859372}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7354838709677419, 'acc_stderr,none': 0.025091892378859275}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.47783251231527096, 'acc_stderr,none': 0.03514528562175008}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.64, 'acc_stderr,none': 0.048241815132442176}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.3851851851851852, 'acc_stderr,none': 0.029670906124630882}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.46357615894039733, 'acc_stderr,none': 0.04071636065944216}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.49537037037037035, 'acc_stderr,none': 0.03409825519163572}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.48214285714285715, 'acc_stderr,none': 0.047427623612430116}}
    },
    "4bit-gptq": {
        "command": """m = Model("TechxGenus/Meta-Llama-3-8B-Instruct-GPTQ", dtype="fp16", add_hooks=False)""",
        "output": {'mmlu': {'acc,none': 0.6157954707306651, 'acc_stderr,none': 0.003859621151344042, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5532412327311371, 'acc_stderr,none': 0.0066843367213675325}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.46825396825396826, 'acc_stderr,none': 0.04463112720677171}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7272727272727273, 'acc_stderr,none': 0.03477691162163659}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8284313725490197, 'acc_stderr,none': 0.026460569561240654}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8312236286919831, 'acc_stderr,none': 0.02438140683258623}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7851239669421488, 'acc_stderr,none': 0.03749492448709699}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252626}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7484662576687117, 'acc_stderr,none': 0.034089978868575295}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6791907514450867, 'acc_stderr,none': 0.025131000233647886}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.25921787709497207, 'acc_stderr,none': 0.014655780837497733}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.7202572347266881, 'acc_stderr,none': 0.025494259350694902}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7160493827160493, 'acc_stderr,none': 0.025089478523765127}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.46153846153846156, 'acc_stderr,none': 0.012732398286190444}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7426900584795322, 'acc_stderr,none': 0.03352799844161865}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.7032507241712262, 'acc_stderr,none': 0.007914947561962394}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252607}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7283018867924528, 'acc_stderr,none': 0.027377706624670713}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6184971098265896, 'acc_stderr,none': 0.03703851193099521}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.4, 'acc_stderr,none': 0.049236596391733084}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6771300448430493, 'acc_stderr,none': 0.03138147637575499}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8446601941747572, 'acc_stderr,none': 0.03586594738573974}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8675213675213675, 'acc_stderr,none': 0.022209309073165616}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.81, 'acc_stderr,none': 0.039427724440366234}, 'mmlu_miscellaneous': {'alias': ' - miscellaneous', 'acc,none': 0.8020434227330779, 'acc_stderr,none': 0.014248873549217582}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.696078431372549, 'acc_stderr,none': 0.026336613469046647}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.49645390070921985, 'acc_stderr,none': 0.02982674915328092}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6985294117647058, 'acc_stderr,none': 0.027875982114273168}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5120481927710844, 'acc_stderr,none': 0.03891364495835817}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7146571335716607, 'acc_stderr,none': 0.00798972769952771}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.4824561403508772, 'acc_stderr,none': 0.04700708033551038}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7676767676767676, 'acc_stderr,none': 0.03008862949021749}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8393782383419689, 'acc_stderr,none': 0.02649905770139742}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6, 'acc_stderr,none': 0.024838811988033168}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.6764705882352942, 'acc_stderr,none': 0.030388353551886793}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.7981651376146789, 'acc_stderr,none': 0.01720857935778757}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7709923664122137, 'acc_stderr,none': 0.036853466317118506}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6584967320261438, 'acc_stderr,none': 0.019184639328092487}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.710204081632653, 'acc_stderr,none': 0.02904308868330435}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8258706467661692, 'acc_stderr,none': 0.026814951200421603}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.83, 'acc_stderr,none': 0.0377525168068637}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5264827148747225, 'acc_stderr,none': 0.00862213163222025}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.31, 'acc_stderr,none': 0.04648231987117316}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.5925925925925926, 'acc_stderr,none': 0.04244633238353228}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6447368421052632, 'acc_stderr,none': 0.03894734487013317}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7222222222222222, 'acc_stderr,none': 0.03745554791462457}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.43, 'acc_stderr,none': 0.049756985195624284}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.41, 'acc_stderr,none': 0.04943110704237102}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.27, 'acc_stderr,none': 0.04461960433384741}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.49019607843137253, 'acc_stderr,none': 0.04974229460422817}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.77, 'acc_stderr,none': 0.04229525846816506}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5531914893617021, 'acc_stderr,none': 0.0325005368436584}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6068965517241379, 'acc_stderr,none': 0.040703290137070705}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.455026455026455, 'acc_stderr,none': 0.025646928361049395}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7258064516129032, 'acc_stderr,none': 0.02537813997088519}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.5024630541871922, 'acc_stderr,none': 0.03517945038691063}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.61, 'acc_stderr,none': 0.04902071300001974}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.37407407407407406, 'acc_stderr,none': 0.02950286112895529}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.47019867549668876, 'acc_stderr,none': 0.040752249922169775}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.4722222222222222, 'acc_stderr,none': 0.0340470532865388}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.5089285714285714, 'acc_stderr,none': 0.04745033255489123}}
    },
    "4bit-nf": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="nf4", collect_midlayers=False)""",
        "output": {'mmlu': {'acc,none': 0.6144423871243413, 'acc_stderr,none': 0.003870266747448727, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5511158342189161, 'acc_stderr,none': 0.0067569191737127466}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.46825396825396826, 'acc_stderr,none': 0.04463112720677172}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7272727272727273, 'acc_stderr,none': 0.03477691162163659}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.7990196078431373, 'acc_stderr,none': 0.028125972265654362}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.810126582278481, 'acc_stderr,none': 0.02553010046023351}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7107438016528925, 'acc_stderr,none': 0.041391127276354626}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.75, 'acc_stderr,none': 0.04186091791394607}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7423312883435583, 'acc_stderr,none': 0.03436150827846917}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.684971098265896, 'acc_stderr,none': 0.025009313790069716}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.27932960893854747, 'acc_stderr,none': 0.015005762446786171}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.6913183279742765, 'acc_stderr,none': 0.02623696588115326}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7191358024691358, 'acc_stderr,none': 0.025006469755799208}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4595827900912647, 'acc_stderr,none': 0.012728446067669971}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7660818713450293, 'acc_stderr,none': 0.03246721765117826}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.7003540392661731, 'acc_stderr,none': 0.007942092350031394}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.68, 'acc_stderr,none': 0.04688261722621504}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7245283018867924, 'acc_stderr,none': 0.02749566368372405}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6242774566473989, 'acc_stderr,none': 0.036928207672648664}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.4, 'acc_stderr,none': 0.049236596391733084}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6457399103139013, 'acc_stderr,none': 0.03210062154134987}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8058252427184466, 'acc_stderr,none': 0.03916667762822584}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8803418803418803, 'acc_stderr,none': 0.021262719400406953}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.81, 'acc_stderr,none': 0.03942772444036623}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7905491698595147, 'acc_stderr,none': 0.014551310568143705}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7091503267973857, 'acc_stderr,none': 0.02600480036395213}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.49645390070921985, 'acc_stderr,none': 0.02982674915328092}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.7169117647058824, 'acc_stderr,none': 0.02736586113151381}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5, 'acc_stderr,none': 0.03892494720807614}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7188820279493012, 'acc_stderr,none': 0.007927063168668808}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.4298245614035088, 'acc_stderr,none': 0.04657047260594964}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7727272727272727, 'acc_stderr,none': 0.02985751567338641}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8393782383419689, 'acc_stderr,none': 0.026499057701397433}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6102564102564103, 'acc_stderr,none': 0.024726967886647078}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.726890756302521, 'acc_stderr,none': 0.028942004040998167}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.818348623853211, 'acc_stderr,none': 0.016530617409266875}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7633587786259542, 'acc_stderr,none': 0.037276735755969154}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6552287581699346, 'acc_stderr,none': 0.019228322018696647}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6454545454545455, 'acc_stderr,none': 0.045820048415054174}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.7061224489795919, 'acc_stderr,none': 0.02916273841024976}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8208955223880597, 'acc_stderr,none': 0.027113286753111837}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.81, 'acc_stderr,none': 0.03942772444036623}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5223596574690771, 'acc_stderr,none': 0.008621637250769507}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.31, 'acc_stderr,none': 0.04648231987117316}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6, 'acc_stderr,none': 0.04232073695151589}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6644736842105263, 'acc_stderr,none': 0.03842498559395269}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7361111111111112, 'acc_stderr,none': 0.03685651095897532}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.42, 'acc_stderr,none': 0.049604496374885836}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.47, 'acc_stderr,none': 0.05016135580465919}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.3, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.49019607843137253, 'acc_stderr,none': 0.04974229460422817}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5319148936170213, 'acc_stderr,none': 0.03261936918467382}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6275862068965518, 'acc_stderr,none': 0.04028731532947558}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.4444444444444444, 'acc_stderr,none': 0.025591857761382182}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7064516129032258, 'acc_stderr,none': 0.025906087021319288}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.458128078817734, 'acc_stderr,none': 0.03505630140785741}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.62, 'acc_stderr,none': 0.048783173121456316}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.34444444444444444, 'acc_stderr,none': 0.02897264888484427}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.45695364238410596, 'acc_stderr,none': 0.04067325174247443}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.5092592592592593, 'acc_stderr,none': 0.03409386946992699}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.48214285714285715, 'acc_stderr,none': 0.047427623612430116}}
    },
    "4bit-bnb": {
        "command": """m = Model("NousResearch/Meta-Llama-3-8B-Instruct", dtype="int4", add_hooks=False, model_device="cuda")""",
        "output": {'mmlu': {'acc,none': 0.608033043725965, 'acc_stderr,none': 0.0039018783253919214, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5528161530286929, 'acc_stderr,none': 0.0067612807213922755}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.4444444444444444, 'acc_stderr,none': 0.044444444444444495}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.696969696969697, 'acc_stderr,none': 0.03588624800091707}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8333333333333334, 'acc_stderr,none': 0.026156867523931045}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8312236286919831, 'acc_stderr,none': 0.024381406832586227}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7768595041322314, 'acc_stderr,none': 0.03800754475228733}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7129629629629629, 'acc_stderr,none': 0.043733130409147614}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7300613496932515, 'acc_stderr,none': 0.034878251684978906}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6734104046242775, 'acc_stderr,none': 0.025248264774242822}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.29720670391061454, 'acc_stderr,none': 0.015285313353641599}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.707395498392283, 'acc_stderr,none': 0.02583989833487798}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7067901234567902, 'acc_stderr,none': 0.025329888171900926}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.45371577574967403, 'acc_stderr,none': 0.012715404841277738}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7543859649122807, 'acc_stderr,none': 0.03301405946987251}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.6842613453492115, 'acc_stderr,none': 0.00807318172654706}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252609}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.6830188679245283, 'acc_stderr,none': 0.028637235639800893}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6184971098265896, 'acc_stderr,none': 0.03703851193099521}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.41, 'acc_stderr,none': 0.04943110704237102}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6636771300448431, 'acc_stderr,none': 0.031708824268455}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8349514563106796, 'acc_stderr,none': 0.036756688322331886}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8632478632478633, 'acc_stderr,none': 0.0225090339370778}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.73, 'acc_stderr,none': 0.044619604333847415}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7867177522349936, 'acc_stderr,none': 0.014648172749593522}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.6797385620915033, 'acc_stderr,none': 0.02671611838015683}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.4787234042553192, 'acc_stderr,none': 0.029800481645628693}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6617647058823529, 'acc_stderr,none': 0.02873932851398358}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.4939759036144578, 'acc_stderr,none': 0.03892212195333045}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.704582385440364, 'acc_stderr,none': 0.00806606879111788}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.43859649122807015, 'acc_stderr,none': 0.04668000738510455}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7323232323232324, 'acc_stderr,none': 0.03154449888270287}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8341968911917098, 'acc_stderr,none': 0.026839845022314415}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6307692307692307, 'acc_stderr,none': 0.02446861524147892}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.6554621848739496, 'acc_stderr,none': 0.030868682604121626}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8073394495412844, 'acc_stderr,none': 0.016909276884936087}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.732824427480916, 'acc_stderr,none': 0.03880848301082397}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6372549019607843, 'acc_stderr,none': 0.019450768432505518}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6454545454545455, 'acc_stderr,none': 0.045820048415054174}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.6857142857142857, 'acc_stderr,none': 0.029719329422417465}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8109452736318408, 'acc_stderr,none': 0.02768691358801301}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.82, 'acc_stderr,none': 0.03861229196653694}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5210910244211862, 'acc_stderr,none': 0.008655555543335523}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.36, 'acc_stderr,none': 0.04824181513244218}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6, 'acc_stderr,none': 0.04232073695151589}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6644736842105263, 'acc_stderr,none': 0.0384249855939527}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.6944444444444444, 'acc_stderr,none': 0.03852084696008534}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.41, 'acc_stderr,none': 0.049431107042371025}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.5, 'acc_stderr,none': 0.050251890762960605}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.32, 'acc_stderr,none': 0.046882617226215034}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.43137254901960786, 'acc_stderr,none': 0.04928099597287534}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.7, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5531914893617021, 'acc_stderr,none': 0.0325005368436584}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6068965517241379, 'acc_stderr,none': 0.040703290137070705}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.4312169312169312, 'acc_stderr,none': 0.0255064816981382}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7161290322580646, 'acc_stderr,none': 0.025649381063029275}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.46798029556650245, 'acc_stderr,none': 0.03510766597959217}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.65, 'acc_stderr,none': 0.0479372485441102}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.362962962962963, 'acc_stderr,none': 0.02931820364520686}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.48344370860927155, 'acc_stderr,none': 0.0408024418562897}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.4583333333333333, 'acc_stderr,none': 0.033981108902946366}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.49107142857142855, 'acc_stderr,none': 0.04745033255489123}}
    },
    "4bit-awq": {
        # "command": """m = Model("kaitchup/Llama-3-8b-awq-4bit", dtype="fp16", add_hooks=False)""",
        # "output": {'mmlu': {'acc,none': 0.6011251958410483, 'acc_stderr,none': 0.003879720496926495, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5349628055260361, 'acc_stderr,none': 0.006741015864725949}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.3968253968253968, 'acc_stderr,none': 0.043758884927270605}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7151515151515152, 'acc_stderr,none': 0.03524390844511781}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.7892156862745098, 'acc_stderr,none': 0.0286265479124374}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8143459915611815, 'acc_stderr,none': 0.02531049537694487}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.743801652892562, 'acc_stderr,none': 0.03984979653302872}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7222222222222222, 'acc_stderr,none': 0.04330043749650742}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.6993865030674846, 'acc_stderr,none': 0.0360251131880677}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6734104046242775, 'acc_stderr,none': 0.025248264774242832}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.26033519553072626, 'acc_stderr,none': 0.01467625200931947}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.6655948553054662, 'acc_stderr,none': 0.026795422327893944}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7067901234567902, 'acc_stderr,none': 0.02532988817190092}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.44002607561929596, 'acc_stderr,none': 0.012678037478574515}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7953216374269005, 'acc_stderr,none': 0.030944459778533204}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.6887672996459607, 'acc_stderr,none': 0.00798553931086757}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.66, 'acc_stderr,none': 0.04760952285695237}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.690566037735849, 'acc_stderr,none': 0.028450154794118634}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6242774566473989, 'acc_stderr,none': 0.036928207672648664}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.33, 'acc_stderr,none': 0.047258156262526045}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6636771300448431, 'acc_stderr,none': 0.031708824268455005}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.7961165048543689, 'acc_stderr,none': 0.0398913985953177}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8589743589743589, 'acc_stderr,none': 0.022801382534597524}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.79, 'acc_stderr,none': 0.040936018074033256}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.8058748403575989, 'acc_stderr,none': 0.01414397027665757}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7058823529411765, 'acc_stderr,none': 0.02609016250427904}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.46808510638297873, 'acc_stderr,none': 0.029766675075873873}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6470588235294118, 'acc_stderr,none': 0.029029422815681407}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5120481927710844, 'acc_stderr,none': 0.03891364495835816}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7062073448163796, 'acc_stderr,none': 0.008017647251467171}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.37719298245614036, 'acc_stderr,none': 0.04559522141958216}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7424242424242424, 'acc_stderr,none': 0.03115626951964684}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8393782383419689, 'acc_stderr,none': 0.026499057701397422}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.5871794871794872, 'acc_stderr,none': 0.0249626835643318}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.6764705882352942, 'acc_stderr,none': 0.030388353551886797}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.7981651376146789, 'acc_stderr,none': 0.01720857935778755}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.732824427480916, 'acc_stderr,none': 0.03880848301082396}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6715686274509803, 'acc_stderr,none': 0.018999707383162662}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6545454545454545, 'acc_stderr,none': 0.04554619617541054}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.6857142857142857, 'acc_stderr,none': 0.029719329422417468}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8109452736318408, 'acc_stderr,none': 0.027686913588013024}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.86, 'acc_stderr,none': 0.03487350880197769}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.510941960038059, 'acc_stderr,none': 0.008614893683651759}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.3, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.5777777777777777, 'acc_stderr,none': 0.04266763404099582}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6447368421052632, 'acc_stderr,none': 0.03894734487013316}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.75, 'acc_stderr,none': 0.03621034121889507}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.44, 'acc_stderr,none': 0.04988876515698589}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.44, 'acc_stderr,none': 0.04988876515698589}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.33, 'acc_stderr,none': 0.04725815626252605}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.37254901960784315, 'acc_stderr,none': 0.048108401480826346}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5148936170212766, 'acc_stderr,none': 0.032671518489247764}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.5448275862068965, 'acc_stderr,none': 0.04149886942192117}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.41534391534391535, 'acc_stderr,none': 0.025379524910778398}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7290322580645161, 'acc_stderr,none': 0.02528441611490016}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.49261083743842365, 'acc_stderr,none': 0.03517603540361006}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.62, 'acc_stderr,none': 0.04878317312145632}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.37777777777777777, 'acc_stderr,none': 0.02956070739246571}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.4370860927152318, 'acc_stderr,none': 0.04050035722230636}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.48148148148148145, 'acc_stderr,none': 0.03407632093854052}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.4107142857142857, 'acc_stderr,none': 0.04669510663875191}}
        "command": """m = Model("TechxGenus/Meta-Llama-3-8B-Instruct-AWQ", dtype="fp16", add_hooks=False, device_map="cuda")"""
        "output": {'time': 11179.254959106445, 'mmlu': {'acc,none': 0.6184304230166643, 'acc_stderr,none': 0.0038791804520669085, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5619553666312433, 'acc_stderr,none': 0.006793094900982456}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.4444444444444444, 'acc_stderr,none': 0.04444444444444449}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7454545454545455, 'acc_stderr,none': 0.03401506715249039}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8088235294117647, 'acc_stderr,none': 0.027599174300640773}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.7974683544303798, 'acc_stderr,none': 0.02616056824660147}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7272727272727273, 'acc_stderr,none': 0.04065578140908705}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252627}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7791411042944786, 'acc_stderr,none': 0.03259177392742178}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6965317919075145, 'acc_stderr,none': 0.024752411960917205}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.3206703910614525, 'acc_stderr,none': 0.015609929559348397}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.6945337620578779, 'acc_stderr,none': 0.026160584450140457}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.6975308641975309, 'acc_stderr,none': 0.025557653981868034}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.46479791395045633, 'acc_stderr,none': 0.012738547371303956}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7602339181286549, 'acc_stderr,none': 0.03274485211946956}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.6990666237528163, 'acc_stderr,none': 0.007940841261518075}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.63, 'acc_stderr,none': 0.04852365870939099}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.720754716981132, 'acc_stderr,none': 0.027611163402399715}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6589595375722543, 'acc_stderr,none': 0.036146654241808254}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.39, 'acc_stderr,none': 0.04902071300001974}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6681614349775785, 'acc_stderr,none': 0.03160295143776679}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8349514563106796, 'acc_stderr,none': 0.036756688322331886}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8717948717948718, 'acc_stderr,none': 0.02190190511507333}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.81, 'acc_stderr,none': 0.039427724440366234}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7969348659003831, 'acc_stderr,none': 0.014385525076611571}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7091503267973857, 'acc_stderr,none': 0.02600480036395213}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.48936170212765956, 'acc_stderr,none': 0.029820747191422466}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6727941176470589, 'acc_stderr,none': 0.02850145286039657}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5, 'acc_stderr,none': 0.03892494720807614}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7198570035749107, 'acc_stderr,none': 0.007949159578074693}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.47368421052631576, 'acc_stderr,none': 0.046970851366478626}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7474747474747475, 'acc_stderr,none': 0.030954055470365907}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8497409326424871, 'acc_stderr,none': 0.02578772318072386}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6435897435897436, 'acc_stderr,none': 0.0242831405294673}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.6680672268907563, 'acc_stderr,none': 0.030588697013783642}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8073394495412844, 'acc_stderr,none': 0.016909276884936077}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.732824427480916, 'acc_stderr,none': 0.03880848301082396}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6584967320261438, 'acc_stderr,none': 0.019184639328092484}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6545454545454545, 'acc_stderr,none': 0.04554619617541054}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.7142857142857143, 'acc_stderr,none': 0.028920583220675578}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8407960199004975, 'acc_stderr,none': 0.02587064676616914}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.84, 'acc_stderr,none': 0.036845294917747066}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5242626070409134, 'acc_stderr,none': 0.008619530365036954}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.35, 'acc_stderr,none': 0.0479372485441102}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6148148148148148, 'acc_stderr,none': 0.042039210401562783}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.7039473684210527, 'acc_stderr,none': 0.03715062154998904}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7152777777777778, 'acc_stderr,none': 0.03773809990686935}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.4, 'acc_stderr,none': 0.049236596391733084}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.49, 'acc_stderr,none': 0.05024183937956912}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.36, 'acc_stderr,none': 0.048241815132442176}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.43137254901960786, 'acc_stderr,none': 0.04928099597287534}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.74, 'acc_stderr,none': 0.044084400227680794}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5531914893617021, 'acc_stderr,none': 0.0325005368436584}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6068965517241379, 'acc_stderr,none': 0.040703290137070705}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.4312169312169312, 'acc_stderr,none': 0.02550648169813821}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7193548387096774, 'acc_stderr,none': 0.025560604721022895}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.4482758620689655, 'acc_stderr,none': 0.03499113137676744}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252609}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.37407407407407406, 'acc_stderr,none': 0.02950286112895529}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.4304635761589404, 'acc_stderr,none': 0.04042809961395634}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.4583333333333333, 'acc_stderr,none': 0.033981108902946366}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.49107142857142855, 'acc_stderr,none': 0.04745033255489123}},
    },
    "3bit-hqq": {
        "output": {'mmlu': {'acc,none': 0.6225608887622845, 'acc_stderr,none': 0.003873462460308763, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5687566418703507, 'acc_stderr,none': 0.006787652258090692}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.48412698412698413, 'acc_stderr,none': 0.04469881854072606}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.7212121212121212, 'acc_stderr,none': 0.03501438706296781}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.7990196078431373, 'acc_stderr,none': 0.028125972265654362}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.8227848101265823, 'acc_stderr,none': 0.024856364184503234}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.7603305785123967, 'acc_stderr,none': 0.038968789850704164}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7685185185185185, 'acc_stderr,none': 0.04077494709252627}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.7423312883435583, 'acc_stderr,none': 0.03436150827846917}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6994219653179191, 'acc_stderr,none': 0.024685316867257803}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.33631284916201115, 'acc_stderr,none': 0.015801003729145897}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.7266881028938906, 'acc_stderr,none': 0.025311765975426115}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.7129629629629629, 'acc_stderr,none': 0.025171041915309684}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.4634941329856584, 'acc_stderr,none': 0.012736153390214961}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.7660818713450293, 'acc_stderr,none': 0.03246721765117826}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.6990666237528163, 'acc_stderr,none': 0.007981226714906503}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.67, 'acc_stderr,none': 0.04725815626252607}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.7056603773584905, 'acc_stderr,none': 0.028049186315695238}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.6473988439306358, 'acc_stderr,none': 0.03643037168958548}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.38, 'acc_stderr,none': 0.048783173121456316}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.672645739910314, 'acc_stderr,none': 0.031493846709941306}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8058252427184466, 'acc_stderr,none': 0.03916667762822582}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8717948717948718, 'acc_stderr,none': 0.021901905115073318}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.79, 'acc_stderr,none': 0.040936018074033256}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7879948914431673, 'acc_stderr,none': 0.014616099385833695}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.7091503267973857, 'acc_stderr,none': 0.02600480036395213}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.524822695035461, 'acc_stderr,none': 0.029790719243829714}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6875, 'acc_stderr,none': 0.02815637344037142}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.5, 'acc_stderr,none': 0.03892494720807614}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.7240818979525512, 'acc_stderr,none': 0.007857255216466498}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.42105263157894735, 'acc_stderr,none': 0.046446020912223177}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.7777777777777778, 'acc_stderr,none': 0.02962022787479047}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.8704663212435233, 'acc_stderr,none': 0.024233532297758712}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.6230769230769231, 'acc_stderr,none': 0.024570975364225995}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.6890756302521008, 'acc_stderr,none': 0.030066761582977934}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.8238532110091743, 'acc_stderr,none': 0.01633288239343138}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.732824427480916, 'acc_stderr,none': 0.038808483010823965}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6552287581699346, 'acc_stderr,none': 0.019228322018696644}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6636363636363637, 'acc_stderr,none': 0.04525393596302505}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.726530612244898, 'acc_stderr,none': 0.028535560337128445}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8407960199004975, 'acc_stderr,none': 0.02587064676616914}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.85, 'acc_stderr,none': 0.035887028128263714}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.5283856644465589, 'acc_stderr,none': 0.00862210767632489}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.3, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.6074074074074074, 'acc_stderr,none': 0.04218506215368879}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.6447368421052632, 'acc_stderr,none': 0.03894734487013316}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.7430555555555556, 'acc_stderr,none': 0.03653946969442099}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.4, 'acc_stderr,none': 0.049236596391733084}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.54, 'acc_stderr,none': 0.05009082659620333}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.34, 'acc_stderr,none': 0.04760952285695235}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.47058823529411764, 'acc_stderr,none': 0.049665709039785295}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.75, 'acc_stderr,none': 0.04351941398892446}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.5404255319148936, 'acc_stderr,none': 0.03257901482099835}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.6275862068965518, 'acc_stderr,none': 0.04028731532947558}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.4365079365079365, 'acc_stderr,none': 0.025542846817400506}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.7290322580645161, 'acc_stderr,none': 0.025284416114900156}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.4729064039408867, 'acc_stderr,none': 0.03512819077876106}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.63, 'acc_stderr,none': 0.04852365870939098}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.3851851851851852, 'acc_stderr,none': 0.029670906124630882}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.46357615894039733, 'acc_stderr,none': 0.04071636065944216}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.48148148148148145, 'acc_stderr,none': 0.034076320938540516}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.4642857142857143, 'acc_stderr,none': 0.04733667890053756}},
        "output_non_instruct_i_think": {'mmlu': {'acc,none': 0.5739210938612733, 'acc_stderr,none': 0.003952622877876883, 'alias': 'mmlu'}, 'mmlu_humanities': {'alias': ' - humanities', 'acc,none': 0.5164718384697131, 'acc_stderr,none': 0.0068115134809108935}, 'mmlu_formal_logic': {'alias': '  - formal_logic', 'acc,none': 0.4444444444444444, 'acc_stderr,none': 0.04444444444444449}, 'mmlu_high_school_european_history': {'alias': '  - high_school_european_history', 'acc,none': 0.703030303030303, 'acc_stderr,none': 0.0356796977226805}, 'mmlu_high_school_us_history': {'alias': '  - high_school_us_history', 'acc,none': 0.8137254901960784, 'acc_stderr,none': 0.02732547096671633}, 'mmlu_high_school_world_history': {'alias': '  - high_school_world_history', 'acc,none': 0.7426160337552743, 'acc_stderr,none': 0.028458820991460288}, 'mmlu_international_law': {'alias': '  - international_law', 'acc,none': 0.743801652892562, 'acc_stderr,none': 0.03984979653302872}, 'mmlu_jurisprudence': {'alias': '  - jurisprudence', 'acc,none': 0.7037037037037037, 'acc_stderr,none': 0.04414343666854932}, 'mmlu_logical_fallacies': {'alias': '  - logical_fallacies', 'acc,none': 0.6748466257668712, 'acc_stderr,none': 0.03680350371286461}, 'mmlu_moral_disputes': {'alias': '  - moral_disputes', 'acc,none': 0.6473988439306358, 'acc_stderr,none': 0.025722802200895813}, 'mmlu_moral_scenarios': {'alias': '  - moral_scenarios', 'acc,none': 0.23687150837988827, 'acc_stderr,none': 0.014219570788103982}, 'mmlu_philosophy': {'alias': '  - philosophy', 'acc,none': 0.6430868167202572, 'acc_stderr,none': 0.027210420375934012}, 'mmlu_prehistory': {'alias': '  - prehistory', 'acc,none': 0.6450617283950617, 'acc_stderr,none': 0.026624152478845847}, 'mmlu_professional_law': {'alias': '  - professional_law', 'acc,none': 0.44132985658409385, 'acc_stderr,none': 0.012682016335646674}, 'mmlu_world_religions': {'alias': '  - world_religions', 'acc,none': 0.6900584795321637, 'acc_stderr,none': 0.035469769593931624}, 'mmlu_other': {'alias': ' - other', 'acc,none': 0.654650788542002, 'acc_stderr,none': 0.008251154767686411}, 'mmlu_business_ethics': {'alias': '  - business_ethics', 'acc,none': 0.66, 'acc_stderr,none': 0.04760952285695237}, 'mmlu_clinical_knowledge': {'alias': '  - clinical_knowledge', 'acc,none': 0.6415094339622641, 'acc_stderr,none': 0.029514703583981772}, 'mmlu_college_medicine': {'alias': '  - college_medicine', 'acc,none': 0.5953757225433526, 'acc_stderr,none': 0.0374246119388725}, 'mmlu_global_facts': {'alias': '  - global_facts', 'acc,none': 0.41, 'acc_stderr,none': 0.049431107042371025}, 'mmlu_human_aging': {'alias': '  - human_aging', 'acc,none': 0.6367713004484304, 'acc_stderr,none': 0.032277904428505}, 'mmlu_management': {'alias': '  - management', 'acc,none': 0.8252427184466019, 'acc_stderr,none': 0.037601780060266196}, 'mmlu_marketing': {'alias': '  - marketing', 'acc,none': 0.8461538461538461, 'acc_stderr,none': 0.023636873317489284}, 'mmlu_medical_genetics': {'alias': '  - medical_genetics', 'acc,none': 0.7, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_miscellaneous': {'alias': '  - miscellaneous', 'acc,none': 0.7624521072796935, 'acc_stderr,none': 0.015218733046150191}, 'mmlu_nutrition': {'alias': '  - nutrition', 'acc,none': 0.6176470588235294, 'acc_stderr,none': 0.02782610930728369}, 'mmlu_professional_accounting': {'alias': '  - professional_accounting', 'acc,none': 0.43617021276595747, 'acc_stderr,none': 0.02958345203628407}, 'mmlu_professional_medicine': {'alias': '  - professional_medicine', 'acc,none': 0.6397058823529411, 'acc_stderr,none': 0.029163128570670733}, 'mmlu_virology': {'alias': '  - virology', 'acc,none': 0.4578313253012048, 'acc_stderr,none': 0.03878626771002361}, 'mmlu_social_sciences': {'alias': ' - social_sciences', 'acc,none': 0.6750081247968801, 'acc_stderr,none': 0.008267312815307215}, 'mmlu_econometrics': {'alias': '  - econometrics', 'acc,none': 0.39473684210526316, 'acc_stderr,none': 0.045981880578165414}, 'mmlu_high_school_geography': {'alias': '  - high_school_geography', 'acc,none': 0.6717171717171717, 'acc_stderr,none': 0.03345678422756776}, 'mmlu_high_school_government_and_politics': {'alias': '  - high_school_government_and_politics', 'acc,none': 0.7772020725388601, 'acc_stderr,none': 0.030031147977641535}, 'mmlu_high_school_macroeconomics': {'alias': '  - high_school_macroeconomics', 'acc,none': 0.5743589743589743, 'acc_stderr,none': 0.02506909438729654}, 'mmlu_high_school_microeconomics': {'alias': '  - high_school_microeconomics', 'acc,none': 0.5966386554621849, 'acc_stderr,none': 0.031866081214088314}, 'mmlu_high_school_psychology': {'alias': '  - high_school_psychology', 'acc,none': 0.7908256880733945, 'acc_stderr,none': 0.017437937173343226}, 'mmlu_human_sexuality': {'alias': '  - human_sexuality', 'acc,none': 0.7175572519083969, 'acc_stderr,none': 0.03948406125768361}, 'mmlu_professional_psychology': {'alias': '  - professional_psychology', 'acc,none': 0.6176470588235294, 'acc_stderr,none': 0.019659922493623347}, 'mmlu_public_relations': {'alias': '  - public_relations', 'acc,none': 0.6272727272727273, 'acc_stderr,none': 0.04631381319425465}, 'mmlu_security_studies': {'alias': '  - security_studies', 'acc,none': 0.710204081632653, 'acc_stderr,none': 0.029043088683304342}, 'mmlu_sociology': {'alias': '  - sociology', 'acc,none': 0.8009950248756219, 'acc_stderr,none': 0.028231365092758406}, 'mmlu_us_foreign_policy': {'alias': '  - us_foreign_policy', 'acc,none': 0.76, 'acc_stderr,none': 0.042923469599092816}, 'mmlu_stem': {'alias': ' - stem', 'acc,none': 0.4814462416745956, 'acc_stderr,none': 0.00868068496719653}, 'mmlu_abstract_algebra': {'alias': '  - abstract_algebra', 'acc,none': 0.31, 'acc_stderr,none': 0.04648231987117316}, 'mmlu_anatomy': {'alias': '  - anatomy', 'acc,none': 0.5481481481481482, 'acc_stderr,none': 0.04299268905480864}, 'mmlu_astronomy': {'alias': '  - astronomy', 'acc,none': 0.625, 'acc_stderr,none': 0.039397364351956274}, 'mmlu_college_biology': {'alias': '  - college_biology', 'acc,none': 0.6736111111111112, 'acc_stderr,none': 0.03921067198982266}, 'mmlu_college_chemistry': {'alias': '  - college_chemistry', 'acc,none': 0.37, 'acc_stderr,none': 0.048523658709391}, 'mmlu_college_computer_science': {'alias': '  - college_computer_science', 'acc,none': 0.44, 'acc_stderr,none': 0.049888765156985884}, 'mmlu_college_mathematics': {'alias': '  - college_mathematics', 'acc,none': 0.32, 'acc_stderr,none': 0.04688261722621504}, 'mmlu_college_physics': {'alias': '  - college_physics', 'acc,none': 0.39215686274509803, 'acc_stderr,none': 0.048580835742663434}, 'mmlu_computer_security': {'alias': '  - computer_security', 'acc,none': 0.7, 'acc_stderr,none': 0.046056618647183814}, 'mmlu_conceptual_physics': {'alias': '  - conceptual_physics', 'acc,none': 0.43829787234042555, 'acc_stderr,none': 0.03243618636108101}, 'mmlu_electrical_engineering': {'alias': '  - electrical_engineering', 'acc,none': 0.5241379310344828, 'acc_stderr,none': 0.0416180850350153}, 'mmlu_elementary_mathematics': {'alias': '  - elementary_mathematics', 'acc,none': 0.42328042328042326, 'acc_stderr,none': 0.025446365634406796}, 'mmlu_high_school_biology': {'alias': '  - high_school_biology', 'acc,none': 0.6806451612903226, 'acc_stderr,none': 0.02652270967466777}, 'mmlu_high_school_chemistry': {'alias': '  - high_school_chemistry', 'acc,none': 0.4236453201970443, 'acc_stderr,none': 0.03476725747649037}, 'mmlu_high_school_computer_science': {'alias': '  - high_school_computer_science', 'acc,none': 0.57, 'acc_stderr,none': 0.049756985195624284}, 'mmlu_high_school_mathematics': {'alias': '  - high_school_mathematics', 'acc,none': 0.36666666666666664, 'acc_stderr,none': 0.029381620726465076}, 'mmlu_high_school_physics': {'alias': '  - high_school_physics', 'acc,none': 0.39072847682119205, 'acc_stderr,none': 0.03983798306659807}, 'mmlu_high_school_statistics': {'alias': '  - high_school_statistics', 'acc,none': 0.4398148148148148, 'acc_stderr,none': 0.0338517797604481}, 'mmlu_machine_learning': {'alias': '  - machine_learning', 'acc,none': 0.4642857142857143, 'acc_stderr,none': 0.04733667890053756}},
    }
}
